{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Store: Azure ML forecasting\n",
    "\n",
    "In this notebook, we'll build and analyze a new model to predict retail sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create working directory\n",
    "\n",
    "The cell below creates our working directory. This will hold our generated scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "project_folder = './scripts'\n",
    "\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/utils.py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def processResults(X_query, y_fcst_all):\n",
    "    temp = X_query[['PRODUCT_NAME','DATE','TEMPERATURE_MEAN']]\n",
    "    temp['DATE'] =  pd.to_datetime(temp['DATE'], format='%Y-%m-%d')\n",
    "    temp['MONTH'] = temp['DATE'].dt.strftime(\"%B\")\n",
    "    temp = temp.drop(columns=['DATE'])\n",
    "    temp = temp.groupby(['MONTH','PRODUCT_NAME'])['TEMPERATURE_MEAN'].mean()\n",
    "    temp = pd.DataFrame(temp).reset_index()\n",
    "    new_order = ['September','October', 'November', 'December']\n",
    "    temp['MONTH'] = pd.Categorical(temp['MONTH'], categories=new_order, ordered=True)\n",
    "    temp.sort_values(by='MONTH',inplace=True)\n",
    "    r, c = y_fcst_all.shape\n",
    "    y_fcst_all['PRODUCT_NAME'] = ['Canned Beans'] * r\n",
    "    y_fcst_all = y_fcst_all[['DATE','PRODUCT_ID','PRODUCT_NAME', 'forecast']]\n",
    "    forecast = y_fcst_all.copy()\n",
    "    forecast['DATE'] =  pd.to_datetime(forecast['DATE'], format='%Y-%m-%d')\n",
    "    forecast = forecast[['PRODUCT_NAME','forecast','DATE']]\n",
    "    forecast['MONTH'] = forecast['DATE'].dt.strftime(\"%B\")\n",
    "    forecast = forecast.drop(columns=['DATE'])\n",
    "    forecast = forecast.groupby(['MONTH','PRODUCT_NAME'])['forecast'].sum()\n",
    "    forecast = pd.DataFrame(forecast).reset_index()\n",
    "    new_order = ['September','October', 'November', 'December']\n",
    "    forecast['MONTH'] = pd.Categorical(forecast['MONTH'], categories=new_order, ordered=True)\n",
    "    forecast.sort_values(by='MONTH',inplace=True)\n",
    "    forecast['TEMPERATURE_MEAN'] = temp['TEMPERATURE_MEAN']\n",
    "    forecast = forecast.dropna()\n",
    "    return forecast \n",
    "    \n",
    "def facetgrid_two_axes(*args, **kwargs):\n",
    "    data = kwargs.pop('data')\n",
    "    dual_axis = kwargs.pop('dual_axis')\n",
    "    alpha = kwargs.pop('alpha', 0.7)\n",
    "    kwargs.pop('color')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    if dual_axis:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.set_ylabel('forecast')\n",
    "        \n",
    "    ax.plot(data['MONTH'],data['TEMPERATURE_MEAN'], **kwargs, color='red',alpha=alpha)\n",
    "    if dual_axis:\n",
    "        sns.barplot(data['MONTH'],data['forecast'], alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Restart\n",
    "\n",
    "Please restart your kernel after the above cell has finished execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Azure ML\n",
    "\n",
    "In the next cell, we create a new Workspace config object using the `<subscription_id>`, `<resource_group_name>`, and `<workspace_name>`. This will fetch the matching Workspace and prompt you for authentication. Please click on the link and input the provided details.\n",
    "\n",
    "For more information on **Workspace**, please visit: [Microsoft Workspace Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py)\n",
    "\n",
    "`<subscription_id>` = You can get this ID from the landing page of your Resource Group.\n",
    "\n",
    "`<resource_group_name>` = This is the name of your Resource Group.\n",
    "\n",
    "`<workspace_name>` = This is the name of your Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "try:    \n",
    "    interactive_auth = InteractiveLoginAuthentication(tenant_id='<tenant_id>')\n",
    "    # Get instance of the Workspace and write it to config file\n",
    "    ws = Workspace(\n",
    "        subscription_id = '<subscription_id>', \n",
    "        resource_group = '<resource_group_name>', \n",
    "        workspace_name = '<workspace_name>',\n",
    "        auth = interactive_auth)\n",
    "\n",
    "    # Writes workspace config file\n",
    "    ws.write_config()\n",
    "    \n",
    "    print('Library configuration succeeded')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "Let's retrieve our dataset from the default workspace Datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "datastore_path = [DataPath(datastore, 'train_data.parquet')]\n",
    "\n",
    "train_tabular = Dataset.Tabular.from_parquet_files(path=datastore_path)\n",
    "train_tabular = train_tabular.register(workspace=ws, \n",
    "                                       name='retail_sales_training',\n",
    "                                       description='Retail sales forecast training data',\n",
    "                                       create_new_version=True)\n",
    "train_tabular = Dataset.get_by_name(ws, 'retail_sales_training')\n",
    "\n",
    "train_data = train_tabular.to_pandas_dataframe()\n",
    "\n",
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take a subset of our data and then proceed to visualize it to better understand any patterns and trends that might exist to drive good ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "Describe our current dataset. The table below shows the different statistical values for our training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = train_tabular.take_sample(probability=0.01, seed=123).to_pandas_dataframe()\n",
    "subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 Canned Beans Sales by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "canned = train_data.loc[train_data['PRODUCT_NAME'] == 'Canned Beans']\n",
    "canned['DATE'] =  pd.to_datetime(canned['DATE'], format='%Y-%m-%d')\n",
    "canned = canned[['PRODUCT_NAME','QUANTITY','DATE']]\n",
    "canned['MONTH'] = canned['DATE'].dt.strftime(\"%B\")\n",
    "canned = canned.loc[canned['DATE'] < '2019-12-31']\n",
    "canned = canned.drop(columns=['DATE'])\n",
    "canned = canned.groupby(['MONTH','PRODUCT_NAME'])['QUANTITY'].sum()\n",
    "canned = pd.DataFrame(canned).reset_index()\n",
    "new_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "canned['MONTH'] = pd.Categorical(canned['MONTH'], categories=new_order, ordered=True)\n",
    "canned.sort_values(by='MONTH',inplace=True)\n",
    "f, ax = plt.subplots(figsize=(14.5, 6.5))\n",
    "ax = sns.barplot(x=\"MONTH\", y=\"QUANTITY\", data=canned)\n",
    "ax.set(ylim=(0, 1600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 Canned Beans Sales by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canned = train_data.loc[train_data['PRODUCT_NAME'] == 'Canned Beans']\n",
    "canned['DATE'] =  pd.to_datetime(canned['DATE'], format='%Y-%m-%d')\n",
    "canned = canned[['PRODUCT_NAME','QUANTITY','DATE']]\n",
    "canned['MONTH'] = canned['DATE'].dt.strftime(\"%B\")\n",
    "canned = canned.loc[canned['DATE'] > '2020-01-01']\n",
    "canned = canned.drop(columns=['DATE'])\n",
    "canned = canned.groupby(['MONTH','PRODUCT_NAME'])['QUANTITY'].sum()\n",
    "canned = pd.DataFrame(canned).reset_index()\n",
    "new_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "canned['MONTH'] = pd.Categorical(canned['MONTH'], categories=new_order, ordered=True)\n",
    "canned.sort_values(by='MONTH',inplace=True)\n",
    "f, ax = plt.subplots(figsize=(14.5, 6.5))\n",
    "ax = sns.barplot(x=\"MONTH\", y=\"QUANTITY\", data=canned)\n",
    "ax.set(ylim=(0, 1600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Workspace Experiment\n",
    "\n",
    "The Experiment constructor allows to create an experiment instance. The constructor takes in the current workspace, which is fetched by calling `Workspace.from_config()` and an experiment name. \n",
    "\n",
    "For more information on **Experiment**, please visit: [Microsoft Experiment Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# Get an instance of the Workspace from the config file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "experiment_name = 'retail-forecast-experiment'\n",
    "\n",
    "# Create Experiment\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure ML Compute cluster\n",
    "\n",
    "Firstly, check for the existence of the cluster. If it already exists, we are able to reuse it. Checking for the existence of the cluster can be performed by calling the constructor `ComputeTarget()` with the current workspace and name of the cluster.\n",
    "\n",
    "In case the cluster does not exist, the next step will be to provide a configuration for the new AML cluster by calling the function `AmlCompute.provisioning_configuration()`. It takes as parameters the VM size and the max number of nodes that the cluster can scale up to. After the configuration has executed, `ComputeTarget.create()` should be called with the previously configuration object and the workspace object.\n",
    "\n",
    "For more information on **ComputeTarget**, please visit: [Microsoft ComputeTarget Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computetarget?view=azure-ml-py)\n",
    "\n",
    "For more information on **AmlCompute**, please visit: [Microsoft AmlCompute Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.akscompute?view=azure-ml-py)\n",
    "\n",
    "\n",
    "**Note:** Please wait for the execution of the cell to finish before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Create AML CPU Compute Cluster\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name='cpucluster')\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS12_v2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, 'cpucluster', compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Experiment\n",
    "\n",
    "We'll use remote compute for this job.\n",
    "\n",
    "The `experiment.submit()` function is called to send the experiment for execution. The only parameter received by this function is the `AutoMLConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "label =  \"QUANTITY\"\n",
    "\n",
    "time_series_settings = {\n",
    "    \"time_column_name\": \"DATE\",\n",
    "    \"grain_column_names\": [\"PRODUCT_ID\"],\n",
    "    \"max_horizon\": 'auto',\n",
    "    \"target_lags\": 'auto',\n",
    "    \"target_rolling_window_size\": 'auto',\n",
    "    \"featurization\": 'auto',\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             experiment_timeout_minutes=15,\n",
    "                             compute_target=compute_target,\n",
    "                             enable_early_stopping=True,\n",
    "                             training_data=train_tabular, #tabular,\n",
    "                             label_column_name=label,\n",
    "                             n_cross_validations=5,\n",
    "                             enable_ensembling=False,\n",
    "                             verbosity=logging.INFO,\n",
    "                             **time_series_settings)\n",
    "\n",
    "run = experiment.submit(automl_config, show_output=False)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Experiment\n",
    "\n",
    "The creation of an object of type `Run` will enable us to observe the experiment progress and results. The object is created by calling the constructor `Run()`. It takes as arguments the experiment and the identifier of the run to fetch. After the object has been instantiated, the `RunDetails()` function will retrieve the progress, metrics, and tasks for the specified run. They will be displayed by calling the function `show()` over the mentioned object.\n",
    "\n",
    "**Note:** Please wait for the execution of the cell to finish before moving forward. (Status should be **Completed**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "run = AutoMLRun(experiment, run.id)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = run.get_output()\n",
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization\n",
    "\n",
    "You can access the engineered feature names generated in time-series featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['timeseriestransformer'].get_engineered_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "featurization_summary = fitted_model.named_steps['timeseriestransformer'].get_featurization_summary()\n",
    "pd.DataFrame.from_records(featurization_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "Next, register the model obtained from the best run. In order to register the model, the function `register_model()` should be called. This will take care of registering the model obtained from the best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = best_run.properties['model_name']\n",
    "\n",
    "description = 'AutoML forecaster'\n",
    "model = run.register_model(model_name = model_name, description = description)\n",
    "\n",
    "print(run.model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the scoring script¶\n",
    "For the deployment we need a function which will run the forecast on serialized data. It can be obtained from the best_run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_name = 'score_fcast.py'\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', script_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model to Azure Container Instance\n",
    "\n",
    "In order to deploy the to an Azure Container Instance, the function `Model.deploy()` should be called, passing along the workspace object, service name and list of models to deploy.\n",
    "\n",
    "For more information on **Model**, please visit: [Microsoft Model Documentation](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py)\n",
    "\n",
    "\n",
    "**Note:** Please wait for the execution of the cell to finish before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "inference_config = InferenceConfig(environment = best_run.get_environment(), \n",
    "                                   entry_script = script_file_name)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n",
    "                                               memory_gb = 2, \n",
    "                                               tags = {'type': \"automl-forecasting\"},\n",
    "                                               description = \"Automl forecasting service\")\n",
    "\n",
    "service_name_aci = 'forecasting-service'\n",
    "print(service_name_aci)\n",
    "\n",
    "try:\n",
    "    aci_service = Webservice(ws, service_name_aci)\n",
    "    print(aci_service.state)\n",
    "except WebserviceException:\n",
    "    aci_service = Model.deploy(ws, service_name_aci, [model], inference_config, aciconfig)\n",
    "    aci_service.wait_for_deployment(True)\n",
    "    print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_path = [DataPath(datastore, 'ntest_data.parquet')]\n",
    "\n",
    "test_tabular = Dataset.Tabular.from_parquet_files(path=datastore_path)\n",
    "test_tabular = test_tabular.register(workspace=ws, \n",
    "                                       name='retail_sales_test',\n",
    "                                       description='Retail sales forecast test data',\n",
    "                                       create_new_version=True)\n",
    "test_tabular = Dataset.get_by_name(ws, 'retail_sales_test')\n",
    "\n",
    "test_data = test_tabular.to_pandas_dataframe()\n",
    "test_labels = test_data.pop(label).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the deployed webservice\n",
    "\n",
    "Now with test data, we can get it into a suitable format to consume the web service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "X_test = test_tabular.to_pandas_dataframe().reset_index(drop=True)\n",
    "y_test = X_test.pop(label).values\n",
    "X_query = X_test.copy()\n",
    "# We have to convert datetime to string, because Timestamps cannot be serialized to JSON.\n",
    "\n",
    "X_query['DATE'] = X_query['DATE'].astype(str)\n",
    "\n",
    "# The Service object accept the complex dictionary, which is internally converted to JSON string.\n",
    "# The section 'data' contains the data frame in the form of dictionary.\n",
    "test_sample = json.dumps({'data': X_query.to_dict(orient='records')})\n",
    "response = aci_service.run(input_data = test_sample)\n",
    "\n",
    "# translate from networkese to datascientese\n",
    "try: \n",
    "    res_dict = json.loads(response)\n",
    "    y_fcst_all = pd.DataFrame(res_dict['index'])\n",
    "    y_fcst_all['DATE'] = pd.to_datetime(y_fcst_all['DATE'], unit = 'ms')\n",
    "    y_fcst_all['forecast'] = res_dict['forecast']    \n",
    "except:\n",
    "    print(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import *\n",
    "\n",
    "forecast = processResults(X_query, y_fcst_all)\n",
    "win_plot = sns.FacetGrid(forecast,  size=8.5)\n",
    "(win_plot.map_dataframe(facetgrid_two_axes, dual_axis=True)\n",
    "         .set_axis_labels(\"MONTH\", \"TEMPERATURE_MEAN\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}